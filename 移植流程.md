官方https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_basic.md
官方的翻译：http://blog.csdn.net/wc781708249/article/details/78596459
简单例子参考https://zhuanlan.zhihu.com/p/23361413
代码集锦（官方里面也有，这里提供中文）：http://blog.csdn.net/wc781708249/article/details/78606514
安装方法集合：  http://blog.csdn.net/wc781708249/article/details/78594750
使用serving的步骤详解（其中对创建client的代码有详细的解析，也有利用docker进行编译和开启tensorflow serving）可参考该训练代码：http://blog.csdn.net/seuer_jeff/article/details/75578732


1，配置环境。bazel和gRCP。
Bazel:集成的类似于makefile 文件性质的编译工具， 
安装参考https://bazel.build/versions/master/docs/install.html 
gRPC:tf_serving 构建client和server连接桥梁的依赖工具，基于protobuf实现的， 
安装参考：https://github.com/grpc/grpc/tree/master/src/python/grpcio 

2，安装serving。通过源代码安装：先下载源代码，再将工作路径切换到serving/tensorflow，然后进行配置。http://www.jishux.com/plus/view-641489-1.html
# GPU版本 将代码中的CPU 改成 GPU
3，导出模型(export)。【训练和模型导出的程序： mnist_saved_model.py】Exporter 已经被官方废弃，有API文档支持 。 SavedModel 格式。此外，为了帮助构造signature defs,SavedModel API提供了  signature def utils工具包utils.py（官方文档里有例子，）。导出的模型包含用于部署的 meta 文件, 模型 checkpoint 文件和序列化的模型 graph（下方有另一种描述，同色）。
如果您想安装tensorflow和tensorflow-serving-api PIP软件包，可以使用简单的python命令运行所有的Python代码（导出和建立客户端）。 要安装PIP包装，请按照此处的说明进行操作（instructions here.）。 也可以使用Bazel来构建必要的依赖关系，并运行所有代码而不安装这些包。
Bazel：
$>bazel build -c opt //tensorflow_serving/example:mnist_saved_model
$>bazel-bin/tensorflow_serving/example/mnist_saved_model /tmp/mnist_model
将训练过的模型导出到/ tmp / mnist_model
或者如果您安装了tensorflow-serving-api，则可以运行：
python tensorflow_serving/example/mnist_saved_model.py /tmp/mnist_model
每个版本的子目录包含以下文件：
saved_model.pb是序列化的tensorflow :: SavedModel。 它包括一个或多个模型的图形定义，以及模型的元数据（如签名）。
变量是包含图形的序列化变量的文件。
注意： 代码中有FLAGS.model_version，其指定模型的版本。 导出相同模型的较新版本时，应该指定较大的整数值。 将创建一个子目录用于导出模型的每个版本，每个版本都将被导出到给定路径下的不同子目录。具有默认值1，因此创建相应的子目录1。

4，部署模型（load）。部署的方式非常简单，只需要以下两步：
$>bazel build -c opt //tensorflow_serving/model_servers:tensorflow_model_server
$>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/   #启动模型服务，监听9000端口
上面是本地编译的，如果您希望跳过编译并使用apt-get进行安装，请按照此处的说明进行操作 instructions here。 然后使用以下命令运行服务器：
tensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/


5，编写创建client来调用部署好的模型。这里我们需要用到 TensorFlow Serving 的 Predict API 和 gRPC 的 implementations.insecure_channel 来construct 一个 request。特别要注意的是 input 的 signature 和数据必须和之前 export 的模型匹配。别忘了配置一下 bazel 的 BUILD 文件

6，运行。通过Bazel运行，或者通过pip软件包运行
$>bazel build -c opt //tensorflow_serving/example:mnist_client
$>bazel-bin/tensorflow_serving/example/mnist_client --num_tests=1000 --server=localhost:9000
如果你安装了PIP package, 也可以运行：
python tensorflow_serving/example/mnist_client.py --num_tests=1000 --server=localhost:9000
